{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findPrime(x):\n",
    "    prime=[2, 3] #\n",
    "    notPrime=[]\n",
    "    numbers=[2,3]\n",
    "    exclude=4 #done to exclude 0 and 1 as they are unique numbers, exclude 2 and 3 as to get the list started.\n",
    "    for i in range(x-exclude+1):\n",
    "        num=i+exclude\n",
    "        status=True\n",
    "        \n",
    "        for j in prime:\n",
    "            if num%j==0:\n",
    "                status=False\n",
    "        numbers.append(num)\n",
    "        if status:\n",
    "            prime.append(num)\n",
    "        else:\n",
    "            notPrime.append(num)\n",
    "    return(prime, notPrime, numbers)\n",
    "\n",
    "number=10000\n",
    "prime, notPrime,numbers=findPrime(number)\n",
    "#print(prime, notPrime)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label=np.zeros((number-1,1))\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in prime:\n",
    "    label[i-2]=1\n",
    "#print(label)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "label_final = np_utils.to_categorical(np.array(label), 2)\n",
    "print(label)\n",
    "print(label_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "data=np.zeros((number-1,1))\n",
    "for i in range(number-1):\n",
    "    data[i]=i+2\n",
    "#print(data)\n",
    "data_final = np_utils.to_categorical(np.array(data), number+1)\n",
    "print(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_final, label_final, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num(x, number):\n",
    "    zeros=np.zeros(number+1)\n",
    "    zeros[x]=1\n",
    "    #print(zeros)\n",
    "    return(zeros)\n",
    "\n",
    "def predPrime(x):\n",
    "    #Prime_model.predict(np.array([num(x, number)]))\n",
    "    #print(Prime_model.predict(np.array([num(x, number)]))[0])\n",
    "    return(Prime_model.predict(np.array([num(x, number)]))[0][0],Prime_model.predict(np.array([num(x, number)]))[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 10,014.0\n",
      "Trainable params: 10,014.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.4653 - acc: 0.8773Epoch 00000: val_loss improved from inf to 0.38361, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00000.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 26s - loss: 0.4648 - acc: 0.8774 - val_loss: 0.3836 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8778Epoch 00001: val_loss improved from 0.38361 to 0.37876, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00000.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 25s - loss: 0.3724 - acc: 0.8779 - val_loss: 0.3788 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8781Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3788 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8778Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3789 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8778Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3792 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 0 , l= 0 , m= 0 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 10,018.0\n",
      "Trainable params: 10,018.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7973/7999 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.8773Epoch 00000: val_loss improved from inf to 0.38444, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00001.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 27s - loss: 0.4693 - acc: 0.8774 - val_loss: 0.3844 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8781Epoch 00001: val_loss improved from 0.38444 to 0.37872, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00001.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 25s - loss: 0.3729 - acc: 0.8779 - val_loss: 0.3787 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8777Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8779Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3791 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8779Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3792 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 0 , l= 0 , m= 1 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 10,017.0\n",
      "Trainable params: 10,017.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7973/7999 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8776Epoch 00000: val_loss improved from inf to 0.38515, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00010.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 27s - loss: 0.4676 - acc: 0.8777 - val_loss: 0.3851 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8779Epoch 00001: val_loss improved from 0.38515 to 0.37873, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00010.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 25s - loss: 0.3730 - acc: 0.8779 - val_loss: 0.3787 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8778Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3788 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8779Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3791 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8782Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3789 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 0 , l= 1 , m= 0 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 10,022.0\n",
      "Trainable params: 10,022.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8775Epoch 00000: val_loss improved from inf to 0.38469, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00011.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 27s - loss: 0.4679 - acc: 0.8774 - val_loss: 0.3847 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8779Epoch 00001: val_loss improved from 0.38469 to 0.37875, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00011.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 25s - loss: 0.3727 - acc: 0.8779 - val_loss: 0.3787 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8776Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3791 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8777Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3792 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8778Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3791 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 0 , l= 1 , m= 1 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 10,017.0\n",
      "Trainable params: 10,017.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8774Epoch 00000: val_loss improved from inf to 0.38633, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00100.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 27s - loss: 0.4706 - acc: 0.8774 - val_loss: 0.3863 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8778Epoch 00001: val_loss improved from 0.38633 to 0.37879, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00100.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 25s - loss: 0.3731 - acc: 0.8779 - val_loss: 0.3788 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8778Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8778Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3792 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8778Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3789 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 1 , l= 0 , m= 0 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 10,021.0\n",
      "Trainable params: 10,021.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8780Epoch 00000: val_loss improved from inf to 0.38576, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00101.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 27s - loss: 0.4696 - acc: 0.8775 - val_loss: 0.3858 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8781Epoch 00001: val_loss improved from 0.38576 to 0.37874, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00101.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 26s - loss: 0.3729 - acc: 0.8779 - val_loss: 0.3787 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8778Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8779Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8779Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 1 , l= 0 , m= 1 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 10,021.0\n",
      "Trainable params: 10,021.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.8781Epoch 00000: val_loss improved from inf to 0.38473, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00110.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 28s - loss: 0.4664 - acc: 0.8775 - val_loss: 0.3847 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "7987/7999 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8782Epoch 00001: val_loss improved from 0.38473 to 0.37873, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00110.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 25s - loss: 0.3727 - acc: 0.8779 - val_loss: 0.3787 - val_acc: 0.8740\n",
      "Epoch 3/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8778Epoch 00002: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "Epoch 4/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8781Epoch 00003: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3713 - acc: 0.8779 - val_loss: 0.3791 - val_acc: 0.8740\n",
      "Epoch 5/100\n",
      "7980/7999 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8781Epoch 00004: val_loss did not improve\n",
      "7999/7999 [==============================] - 25s - loss: 0.3714 - acc: 0.8779 - val_loss: 0.3790 - val_acc: 0.8740\n",
      "7999 7999\n",
      "i= 0 , j= 0 , k= 1 , l= 1 , m= 0 , prime accuracy= 0.0 , not prime accuracy= 100.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_79 (Dense)             (None, 1)                 10002     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 10,026.0\n",
      "Trainable params: 10,026.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "7994/7999 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.8767Epoch 00000: val_loss improved from inf to 0.38517, saving model to G:\\Machine Learning\\Prime-Numbers-master\\weights.bestdense00111.from_scratch.hdf5\n",
      "7999/7999 [==============================] - 28s - loss: 0.4682 - acc: 0.8767 - val_loss: 0.3852 - val_acc: 0.8740\n",
      "Epoch 2/100\n",
      "6174/7999 [======================>.......] - ETA: 5s - loss: 0.3777 - acc: 0.8754"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4abab997509a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m                     Prime_model.fit(X_train, y_train, \n\u001b[0;32m     51\u001b[0m                                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                                     epochs=epochs, batch_size=7, callbacks=[checkpointer, checkpointer2], verbose=1)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[1;31m#load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\icebergChallenge\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#neural network(s):\n",
    "#full:\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping \n",
    "\n",
    "print(X_train.shape[1])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                for m in range(2):\n",
    "                    Prime_model = Sequential()\n",
    "                    Prime_model.add(Dense(2**i, input_shape=(X_train.shape[1],)))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**j, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**k, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**l, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**m, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2, activation='softmax'))\n",
    "                    Prime_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "                    #compile network:\n",
    "                    Prime_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                    #running network:\n",
    "                    \n",
    "                    ### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "                    epochs = 100\n",
    "\n",
    "                    ### Do NOT modify the code below this line.\n",
    "\n",
    "                    checkpointer = ModelCheckpoint(filepath='G:\\Machine Learning\\Prime-Numbers-master\\Dense\\weights.bestdensei'+str(i)+'j'+str(j)+'k'+str(k)+'l'+str(l)+'m'+str(m)+'.from_scratch.hdf5', \n",
    "                                                   verbose=1, save_best_only=True)\n",
    "                    checkpointer2=EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "                    Prime_model.fit(X_train, y_train, \n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    epochs=epochs, batch_size=7, callbacks=[checkpointer, checkpointer2], verbose=1)\n",
    "                    \n",
    "                    #load model\n",
    "                    Prime_model.load_weights('G:\\Machine Learning\\Prime-Numbers-master\\Dense\\weights.bestdensei'+str(i)+'j'+str(j)+'k'+str(k)+'l'+str(l)+'m'+str(m)+'.from_scratch.hdf5')\n",
    "                    \n",
    "                    for n in prime:\n",
    "                        probNotPrime, probPrime= predPrime(n)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                    \n",
    "                    for o in notPrime:\n",
    "                        probNotPrime, probPrime= predPrime(o)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                        \n",
    "                    #data1D=[]\n",
    "                    notPrimeArray=[]\n",
    "                    \n",
    "                    for p in numbers:\n",
    "                        probNotPrime, probPrime=predPrime(p)\n",
    "                        notPrimeArray.append(probNotPrime)\n",
    "                        \n",
    "                        \n",
    "                    labelArray=[]\n",
    "                    for q in label:\n",
    "                        labelArray.append(q[0])\n",
    "\n",
    "                    notPrimeArray2D=[] \n",
    "                    for r in notPrimeArray:\n",
    "                        notPrimeArray2D.append([r])\n",
    "    \n",
    "    \n",
    "                    X_train_boost, X_test_boost, y_train_boost, y_test_boost = train_test_split(notPrimeArray2D, \n",
    "                                                                                                labelArray, test_size=0.2, random_state=42)\n",
    "\n",
    "                    clf=AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "                    print (len(X_train_boost), len(y_train_boost))\n",
    "\n",
    "                    #print (X_train_boost)\n",
    "                    #print (y_train_boost)\n",
    "\n",
    "                    clf.fit(X_train_boost, y_train_boost)\n",
    "                    \n",
    "                    def boostpredict(x):\n",
    "                        return(clf.predict(x))\n",
    "                    \n",
    "                    accuracyPrime=0\n",
    "                    for s in prime:\n",
    "                        probNotPrime, probPrime= predPrime(s)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                        result=boostpredict(probNotPrime)\n",
    "                        #print('i=', i, '  probNotPrime=', result)\n",
    "                        if result==[1.]:\n",
    "                            accuracyPrime+=1\n",
    "                            \n",
    "                    accuracyNotPrime=0\n",
    "                    for t in notPrime:\n",
    "                        probNotPrime, probPrime= predPrime(t)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                        result=boostpredict(probNotPrime)\n",
    "                        #print('i=', i, '  probNotPrime=', result)\n",
    "                        if result==[0.]:\n",
    "                            accuracyNotPrime+=1\n",
    "\n",
    "                            \n",
    "                            \n",
    "                    print('i=', i, ', j=', j, ', k=', k, ', l=', l, ', m=', m,\n",
    "                          ', prime accuracy=', 100.0*accuracyPrime/len(prime),\n",
    "                          ', not prime accuracy=', 100.0*accuracyNotPrime/len(notPrime))\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NN:\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping \n",
    "\n",
    "print(X_train.shape[1])\n",
    "for i in range(13):\n",
    "    for j in range(13):\n",
    "        for k in range(13):\n",
    "            for l in range(13):\n",
    "                for m in range(13):\n",
    "                    Prime_model = Sequential()\n",
    "                    Prime_model.add(Dense(2**i, input_shape=(X_train.shape[1],)))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**j, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**k, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**l, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2**m, activation='relu'))\n",
    "                    Prime_model.add(Dropout(.3))\n",
    "                    Prime_model.add(Dense(2, activation='softmax'))\n",
    "                    Prime_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "                    #compile network:\n",
    "                    Prime_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                    #running network:\n",
    "                    \n",
    "                    ### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "                    epochs = 100\n",
    "\n",
    "                    ### Do NOT modify the code below this line.\n",
    "\n",
    "                    checkpointer = ModelCheckpoint(filepath='G:\\Machine Learning\\Prime-Numbers-master\\Dense\\weights.bestdensei'+str(i)+'j'+str(j)+'k'+str(k)+'l'+str(l)+'m'+str(m)+'.from_scratch.hdf5', \n",
    "                                                   verbose=1, save_best_only=True)\n",
    "                    checkpointer2=EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "                    Prime_model.fit(X_train, y_train, \n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    epochs=epochs, batch_size=7, callbacks=[checkpointer, checkpointer2], verbose=1)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SL(s):\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping \n",
    "\n",
    "print(X_train.shape[1])\n",
    "for i in range(13):\n",
    "    for j in range(13):\n",
    "        for k in range(13):\n",
    "            for l in range(13):\n",
    "                for m in range(13):\n",
    "                                        \n",
    "                    #load model\n",
    "                    Prime_model.load_weights('G:\\Machine Learning\\Prime-Numbers-master\\Dense\\weights.bestdensei'+str(i)+'j'+str(j)+'k'+str(k)+'l'+str(l)+'m'+str(m)+'.from_scratch.hdf5')\n",
    "                    \n",
    "                    for n in prime:\n",
    "                        probNotPrime, probPrime= predPrime(n)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                    \n",
    "                    for o in notPrime:\n",
    "                        probNotPrime, probPrime= predPrime(o)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                        \n",
    "                    #data1D=[]\n",
    "                    notPrimeArray=[]\n",
    "                    \n",
    "                    for p in numbers:\n",
    "                        probNotPrime, probPrime=predPrime(p)\n",
    "                        notPrimeArray.append(probNotPrime)\n",
    "                        \n",
    "                        \n",
    "                    labelArray=[]\n",
    "                    for q in label:\n",
    "                        labelArray.append(q[0])\n",
    "\n",
    "                    notPrimeArray2D=[] \n",
    "                    for r in notPrimeArray:\n",
    "                        notPrimeArray2D.append([r])\n",
    "    \n",
    "    \n",
    "                    X_train_boost, X_test_boost, y_train_boost, y_test_boost = train_test_split(notPrimeArray2D, \n",
    "                                                                                                labelArray, test_size=0.2, random_state=42)\n",
    "\n",
    "                    clf=AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "                    print (len(X_train_boost), len(y_train_boost))\n",
    "\n",
    "                    #print (X_train_boost)\n",
    "                    #print (y_train_boost)\n",
    "\n",
    "                    clf.fit(X_train_boost, y_train_boost)\n",
    "                    \n",
    "                    def boostpredict(x):\n",
    "                        return(clf.predict(x))\n",
    "                    \n",
    "                    accuracyPrime=0\n",
    "                    for s in prime:\n",
    "                        probNotPrime, probPrime= predPrime(s)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                        result=boostpredict(probNotPrime)\n",
    "                        #print('i=', i, '  probNotPrime=', result)\n",
    "                        if result==[1.]:\n",
    "                            accuracyPrime+=1\n",
    "                            \n",
    "                    accuracyNotPrime=0\n",
    "                    for t in notPrime:\n",
    "                        probNotPrime, probPrime= predPrime(t)\n",
    "                        #print('i=', i, '  probNotPrime=', probNotPrime, '  probPrime=', probPrime)\n",
    "                        result=boostpredict(probNotPrime)\n",
    "                        #print('i=', i, '  probNotPrime=', result)\n",
    "                        if result==[0.]:\n",
    "                            accuracyNotPrime+=1\n",
    "\n",
    "                            \n",
    "                            \n",
    "                    print('i=', i, ', j=', j, ', k=', k, ', l=', l, ', m=', m,\n",
    "                          ', prime accuracy=', 100.0*accuracyPrime/len(prime),\n",
    "                          ', not prime accuracy=', 100.0*accuracyNotPrime/len(notPrime))\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:icebergChallenge]",
   "language": "python",
   "name": "conda-env-icebergChallenge-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
